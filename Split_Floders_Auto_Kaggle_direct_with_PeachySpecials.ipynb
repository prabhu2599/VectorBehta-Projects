{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Split Floders Auto - Kaggle direct with PeachySpecials.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhu2599/VectorBehta-Projects/blob/main/Split_Floders_Auto_Kaggle_direct_with_PeachySpecials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-04T04:24:11.727493Z",
          "iopub.execute_input": "2021-07-04T04:24:11.727856Z",
          "iopub.status.idle": "2021-07-04T04:24:11.733615Z",
          "shell.execute_reply.started": "2021-07-04T04:24:11.727780Z",
          "shell.execute_reply": "2021-07-04T04:24:11.732619Z"
        },
        "trusted": true,
        "id": "7U3Bg0DDWCZ8"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:25:03.909210Z",
          "iopub.execute_input": "2021-07-04T04:25:03.909548Z",
          "iopub.status.idle": "2021-07-04T04:25:11.104256Z",
          "shell.execute_reply.started": "2021-07-04T04:25:03.909516Z",
          "shell.execute_reply": "2021-07-04T04:25:11.103378Z"
        },
        "trusted": true,
        "id": "tVAc8vE0WCaA",
        "outputId": "90c6e4a9-0903-450e-befd-15ecf89f39c0"
      },
      "source": [
        "!pip install split-folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:25:18.332419Z",
          "iopub.execute_input": "2021-07-04T04:25:18.332751Z",
          "iopub.status.idle": "2021-07-04T04:25:18.338766Z",
          "shell.execute_reply.started": "2021-07-04T04:25:18.332718Z",
          "shell.execute_reply": "2021-07-04T04:25:18.337984Z"
        },
        "trusted": true,
        "id": "vxO-fhsEWCaC"
      },
      "source": [
        "Data = '../input/sarscov2-ctscan-dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:25:26.710972Z",
          "iopub.execute_input": "2021-07-04T04:25:26.711401Z",
          "iopub.status.idle": "2021-07-04T04:25:46.658822Z",
          "shell.execute_reply.started": "2021-07-04T04:25:26.711363Z",
          "shell.execute_reply": "2021-07-04T04:25:46.657877Z"
        },
        "trusted": true,
        "id": "eBLMUhGrWCaC",
        "outputId": "20c54b92-dcf6-4bc3-d184-4c6a98139a0d"
      },
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio(Data, output=\"output\", seed=1337, ratio=(.8, 0.1,0.1)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 2481 files [00:19, 124.55 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:27:04.575136Z",
          "iopub.execute_input": "2021-07-04T04:27:04.575485Z",
          "iopub.status.idle": "2021-07-04T04:27:05.513597Z",
          "shell.execute_reply.started": "2021-07-04T04:27:04.575455Z",
          "shell.execute_reply": "2021-07-04T04:27:05.512820Z"
        },
        "trusted": true,
        "id": "iMjVkVUYWCaD"
      },
      "source": [
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:27:05.515062Z",
          "iopub.execute_input": "2021-07-04T04:27:05.515378Z",
          "iopub.status.idle": "2021-07-04T04:27:10.434639Z",
          "shell.execute_reply.started": "2021-07-04T04:27:05.515339Z",
          "shell.execute_reply": "2021-07-04T04:27:10.433792Z"
        },
        "trusted": true,
        "id": "j_CE3h-rWCaD"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import zlib\n",
        "import itertools\n",
        "import sklearn\n",
        "import itertools\n",
        "import scipy\n",
        "import skimage\n",
        "from skimage.transform import resize\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import models, layers, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "#from keras.applications.mobilenet import MobileNet\n",
        "#from sklearn.metrics import roc_auc_score\n",
        "#from sklearn.metrics import roc_curve\n",
        "#from sklearn.metrics import auc\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:27:12.378883Z",
          "iopub.execute_input": "2021-07-04T04:27:12.379252Z",
          "iopub.status.idle": "2021-07-04T04:27:19.912982Z",
          "shell.execute_reply.started": "2021-07-04T04:27:12.379218Z",
          "shell.execute_reply": "2021-07-04T04:27:19.911828Z"
        },
        "trusted": true,
        "id": "cMiXYTipWCaE",
        "outputId": "e3112378-b6a6-4e76-eaee-872ee0aeeadd"
      },
      "source": [
        "!pip install efficientnet-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch) (1.7.0)\n",
            "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (0.18.2)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (0.6)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (1.19.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=9513d9683aebd2fbd8843e5c2acbc4a7230ea13e1878cf536dbd74ca1a5f491a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:27:19.916812Z",
          "iopub.execute_input": "2021-07-04T04:27:19.917130Z",
          "iopub.status.idle": "2021-07-04T04:27:21.198617Z",
          "shell.execute_reply.started": "2021-07-04T04:27:19.917098Z",
          "shell.execute_reply": "2021-07-04T04:27:21.197827Z"
        },
        "trusted": true,
        "id": "IWhj_B8TWCaF"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "# from utils import check_accuracy, load_checkpoint, save_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:28:42.775607Z",
          "iopub.execute_input": "2021-07-04T04:28:42.775960Z",
          "iopub.status.idle": "2021-07-04T04:28:42.808934Z",
          "shell.execute_reply.started": "2021-07-04T04:28:42.775928Z",
          "shell.execute_reply": "2021-07-04T04:28:42.806473Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "trusted": true,
        "id": "rOGTlk1tWCaF"
      },
      "source": [
        "def check_accuracy(\n",
        "    loader, model, loss_fn, input_shape=None, toggle_eval=True, print_accuracy=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Check accuracy of model on data from loader\n",
        "    \"\"\"\n",
        "    if toggle_eval:\n",
        "        model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    y_preds = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            if input_shape:\n",
        "                x = x.reshape(x.shape[0], *input_shape)\n",
        "            scores = model(x)\n",
        "            predictions = torch.sigmoid(scores) > 0.5\n",
        "            y_preds.append(torch.clip(torch.sigmoid(scores), 0.005, 0.995).cpu().numpy())\n",
        "            y_true.append(y.cpu().numpy())\n",
        "            num_correct += (predictions.squeeze(1) == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    if toggle_eval:\n",
        "        model.train()\n",
        "\n",
        "    if print_accuracy:\n",
        "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "#         print(log_loss(np.concatenate(y_true, axis=0), np.concatenate(y_preds, axis=0)))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"../working/my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def create_submission(model, model_name, files_dir):\n",
        "    my_transforms = {\n",
        "        \"base\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "        \"horizontal_flip\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.HorizontalFlip(p=1.0),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "        \"vertical_flip\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.VerticalFlip(p=1.0),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "        \"coloring\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.ColorJitter(p=1.0),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "        \"rotate\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.Rotate(p=1.0, limit=45),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "        \"shear\": A.Compose(\n",
        "            [\n",
        "                A.Resize(height=240, width=240),\n",
        "                A.IAAAffine(p=1.0),\n",
        "                A.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225],\n",
        "                    max_pixel_value=255.0,\n",
        "                ),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    for t in [\"base\", \"horizontal_flip\", \"vertical_flip\", \"coloring\", \"rotate\", \"shear\"]:\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        all_files = []\n",
        "        test_dataset = MyDataset(root=files_dir, transform=my_transforms[t])\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=32, num_workers=4, shuffle=False, pin_memory=True\n",
        "        )\n",
        "        model.eval()\n",
        "\n",
        "        for idx, (x, y, filenames) in enumerate(tqdm(test_loader)):\n",
        "            x = x.to(config.DEVICE)\n",
        "            with torch.no_grad():\n",
        "                outputs = (\n",
        "                    torch.clip(torch.sigmoid(model(x)), 0.005, 0.995).squeeze(1).cpu().numpy()\n",
        "                )\n",
        "                predictions.append(outputs)\n",
        "                labels += y.numpy().tolist()\n",
        "                all_files += filenames\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"id\": np.arange(\n",
        "                    1,\n",
        "                    (len(predictions) - 1) * predictions[0].shape[0]\n",
        "                    + predictions[-1].shape[0]\n",
        "                    + 1,\n",
        "                ),\n",
        "                \"label\": np.concatenate(predictions, axis=0),\n",
        "            }\n",
        "        )\n",
        "        df.to_csv(f\"../working/predictions_test/submission_{model_name}_{t}.csv\", index=False)\n",
        "\n",
        "        model.train()\n",
        "        print(f\"Created submission file for model {model_name} and transform {t}\")\n",
        "\n",
        "\n",
        "def save_feature_vectors(model, loader, output_size=(1, 1), file=\"trainb7\"):\n",
        "    model.eval()\n",
        "    images, labels = [], []\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model.extract_features(x)\n",
        "            features = F.adaptive_avg_pool2d(features, output_size=output_size)\n",
        "        images.append(features.reshape(x.shape[0], -1).detach().cpu().numpy())\n",
        "        labels.append(y.numpy())\n",
        "\n",
        "    np.save(f\"../working/X_{file}.npy\", np.concatenate(images, axis=0))\n",
        "    np.save(f\"../working/y_{file}.npy\", np.concatenate(labels, axis=0))\n",
        "    model.train()\n",
        "\n",
        "def train_one_epoch(loader, model, loss_fn, optimizer, scaler):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(DEVICE)\n",
        "        targets = targets.to(DEVICE).unsqueeze(1).float()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            scores = model(data)\n",
        "            loss = loss_fn(scores, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:29:40.227349Z",
          "iopub.execute_input": "2021-07-04T04:29:40.227667Z",
          "iopub.status.idle": "2021-07-04T04:29:40.231804Z",
          "shell.execute_reply.started": "2021-07-04T04:29:40.227636Z",
          "shell.execute_reply": "2021-07-04T04:29:40.230495Z"
        },
        "trusted": true,
        "id": "vrGgXzaxWCaK"
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:29:40.430958Z",
          "iopub.execute_input": "2021-07-04T04:29:40.431327Z",
          "iopub.status.idle": "2021-07-04T04:29:40.437050Z",
          "shell.execute_reply.started": "2021-07-04T04:29:40.431295Z",
          "shell.execute_reply": "2021-07-04T04:29:40.436178Z"
        },
        "trusted": true,
        "id": "9FRb5dT-WCaL"
      },
      "source": [
        "TRAIN_DIR = \"./output/train\"\n",
        "VAL_DIR = \"./output/val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:56.523625Z",
          "iopub.execute_input": "2021-07-04T04:35:56.523955Z",
          "iopub.status.idle": "2021-07-04T04:35:56.531207Z",
          "shell.execute_reply.started": "2021-07-04T04:35:56.523925Z",
          "shell.execute_reply": "2021-07-04T04:35:56.530280Z"
        },
        "trusted": true,
        "id": "eEIzFaNoWCaL"
      },
      "source": [
        "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "import torchvision.transforms as T\n",
        "\n",
        "train_tfms = T.Compose([\n",
        "    T.Resize((256,256)),\n",
        "#    T.CenterCrop(256),\n",
        "#    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "#    T.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "#     T.RandomHorizontalFlip(), \n",
        "#     T.RandomRotation(10),\n",
        "    \n",
        "#    T.Normalize(*imagenet_stats,inplace=True), \n",
        "#    T.RandomErasing(inplace=True),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "valid_tfms = T.Compose([\n",
        "    T.Resize((256,256)),\n",
        "#    T.CenterCrop(256),\n",
        "    T.ToTensor(),\n",
        "#    T.Normalize(*imagenet_stats)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:56.959954Z",
          "iopub.execute_input": "2021-07-04T04:35:56.960300Z",
          "iopub.status.idle": "2021-07-04T04:35:56.965422Z",
          "shell.execute_reply.started": "2021-07-04T04:35:56.960270Z",
          "shell.execute_reply": "2021-07-04T04:35:56.964310Z"
        },
        "trusted": true,
        "id": "0rK0SOKgWCaM"
      },
      "source": [
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 32\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_FILE = \"b7.pth.tar\"\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 2\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:57.429764Z",
          "iopub.execute_input": "2021-07-04T04:35:57.430113Z",
          "iopub.status.idle": "2021-07-04T04:35:57.440782Z",
          "shell.execute_reply.started": "2021-07-04T04:35:57.430076Z",
          "shell.execute_reply": "2021-07-04T04:35:57.439810Z"
        },
        "trusted": true,
        "id": "b9VTdz42WCaM"
      },
      "source": [
        "def call_this_firstuu():\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b7\")\n",
        "    model._fc = nn.Linear(2560, 1)\n",
        "    train_dataset = ImageFolder ( TRAIN_DIR , transform=train_tfms)\n",
        "    test_dataset = ImageFolder ( VAL_DIR , transform=train_tfms ) \n",
        "#     train_dataset = CatDog(root=\"../input/10-monkey-species/training/\", transform=basic_transform)\n",
        "#     test_dataset = CatDog(root=\"../input/10-monkey-species/validation/\", transform=basic_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "    )\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_one_epoch(train_loader, model, loss_fn, optimizer, scaler)\n",
        "        check_accuracy(train_loader, model, loss_fn)\n",
        "\n",
        "    if SAVE_MODEL:\n",
        "        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "        save_checkpoint(checkpoint, filename=CHECKPOINT_FILE)\n",
        "\n",
        "    save_feature_vectors(model, train_loader, output_size=(1, 1), file=\"train_b7\")\n",
        "    save_feature_vectors(model, test_loader, output_size=(1, 1), file=\"test_b7\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:57.911685Z",
          "iopub.execute_input": "2021-07-04T04:35:57.912045Z",
          "iopub.status.idle": "2021-07-04T04:35:58.424386Z",
          "shell.execute_reply.started": "2021-07-04T04:35:57.912007Z",
          "shell.execute_reply": "2021-07-04T04:35:58.423440Z"
        },
        "trusted": true,
        "id": "iZQYAVuYWCaN",
        "outputId": "08771316-bdb4-46c8-bfb7-c1b5b89b7e5e"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9dDafmOWCaN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:58.479137Z",
          "iopub.execute_input": "2021-07-04T04:35:58.479478Z",
          "iopub.status.idle": "2021-07-04T04:35:58.486183Z",
          "shell.execute_reply.started": "2021-07-04T04:35:58.479448Z",
          "shell.execute_reply": "2021-07-04T04:35:58.485068Z"
        },
        "trusted": true,
        "id": "31MxUdCXWCaN"
      },
      "source": [
        "def train_one_epoch(loader, model, loss_fn, optimizer, scaler):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(DEVICE)\n",
        "        targets = targets.to(DEVICE).unsqueeze(1).float()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            scores = model(data)\n",
        "            loss = loss_fn(scores, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:35:59.095814Z",
          "iopub.execute_input": "2021-07-04T04:35:59.096168Z",
          "iopub.status.idle": "2021-07-04T04:39:35.362428Z",
          "shell.execute_reply.started": "2021-07-04T04:35:59.096136Z",
          "shell.execute_reply": "2021-07-04T04:39:35.360855Z"
        },
        "trusted": true,
        "id": "eCucEpNMWCaO",
        "outputId": "7b331ac5-5e2d-4683-fa73-b9d3cf1e6f65"
      },
      "source": [
        "call_this_firstuu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b7\n",
            "=> Loading checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [01:10<00:00,  1.14s/it, loss=0.0302]\n",
            "  0%|          | 0/62 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 92.99%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [01:11<00:00,  1.15s/it, loss=0.021]  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 95.97%\n",
            "=> Saving checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [00:21<00:00,  2.87it/s]\n",
            "100%|██████████| 8/8 [00:04<00:00,  1.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:39:35.364621Z",
          "iopub.execute_input": "2021-07-04T04:39:35.364896Z",
          "iopub.status.idle": "2021-07-04T04:39:35.390651Z",
          "shell.execute_reply.started": "2021-07-04T04:39:35.364868Z",
          "shell.execute_reply": "2021-07-04T04:39:35.389759Z"
        },
        "trusted": true,
        "id": "_eUfEKZqWCaO",
        "outputId": "054504b5-e61c-435f-a0d6-d12f05dee540"
      },
      "source": [
        "X = np.load(f'../working/X_train_b7.npy')\n",
        "y = np.load(f'../working/y_train_b7.npy')\n",
        "\n",
        "print(f\"Training data shape: {X.shape}, labels shape: {y.shape}\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=1337)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape: (1984, 2560), labels shape: (1984,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:39:35.391944Z",
          "iopub.execute_input": "2021-07-04T04:39:35.392315Z",
          "iopub.status.idle": "2021-07-04T04:39:35.794234Z",
          "shell.execute_reply.started": "2021-07-04T04:39:35.392279Z",
          "shell.execute_reply": "2021-07-04T04:39:35.793190Z"
        },
        "trusted": true,
        "id": "xKrz7tdMWCaO",
        "outputId": "9d772c09-d234-41b1-c97c-27ac81a95242"
      },
      "source": [
        "clf = LogisticRegression(max_iter=20000)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-04T04:39:35.798061Z",
          "iopub.execute_input": "2021-07-04T04:39:35.801151Z",
          "iopub.status.idle": "2021-07-04T04:39:35.817809Z",
          "shell.execute_reply.started": "2021-07-04T04:39:35.801103Z",
          "shell.execute_reply": "2021-07-04T04:39:35.816823Z"
        },
        "trusted": true,
        "id": "ZIiJfJOhWCaP",
        "outputId": "d4adfcd9-66fc-4c1f-9ab8-d46eb8f02100"
      },
      "source": [
        "# Check on validation\n",
        "val_preds= clf.predict_proba(X_val)[:,1]\n",
        "print(f\"On validation set:\")\n",
        "print(f\"Accuracy: {clf.score(X_val, y_val)}\")\n",
        "# print(f\"LOG LOSS: {log_loss(y_val, val_preds)} \")\n",
        "# print(\"%--------------------------------------------------%\")\n",
        "\n",
        "# # # Get predictions on test set\n",
        "# # print(\"Getting predictions for test set\")\n",
        "# # X_test = np.load(f'../input/X_test_b7.npy')\n",
        "# # X_test_preds = clf.predict_proba(X_test)[:,1]\n",
        "# # df = pd.DataFrame({'id': np.arange(1, 12501), 'label': np.clip(X_test_preds, 0.005, 0.995)})\n",
        "# # df.to_csv(f\"submissions/mysubmission.csv\", index=False)\n",
        "# # print(\"Done getting predictions!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On validation set:\n",
            "Accuracy: 0.9849246231155779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TC2YodaWCaP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUXO_mvcWCaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}